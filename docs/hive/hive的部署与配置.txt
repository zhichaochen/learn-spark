参考文章：https://blog.csdn.net/tornadojava/article/details/56835459
其中hbase hadoop hive java操作hbase都有。

hive下载地址：http://mirror.bit.edu.cn/apache/hive/hive-2.3.5/
环境
java：jdk1.8
Hadoop: hadoop-2.7.6
Hive: Hive-2.3.5

注意：bin.tar.gz 代表的是编译后的二进制文件。 src.tar.gz:表示源码文件。
===================================================================
1、元数据（metastore）

HIVE的功能是将HQL翻译成MapReduce在Hadoop上执行。
元数据的功能【就是将HQL翻译成MapReduce所需要的数据】。
元数据默认存储在Derby中，建议都用关系型数据库。我的例子是使用了MySql。



=========================================================================
1、准备
    1、下载hive部署包，并解压。
    2、配置hive环境变量，HIVE_HOME，指向hive的目录，并将%HIVE_HOME%\bin添加至环境变量Path。
2、配置文件的修改
    C:\bigdata\apache-hive-2.3.5-src\conf 下的 hive-site.xml配置
    修改value 改成自己的 value。
        <property>
            <name>javax.jdo.option.ConnectionURL</name>
            <value>jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true</value>
            <description>JDBC connect string for a JDBC metastore</description>
        </property>

        <property>
          <name>javax.jdo.option.ConnectionDriverName</name>
          <value>com.mysql.jdbc.Driver</value>
          <description>Driver class name for a JDBC metastore</description>
        </property>

        <property>
          <name>javax.jdo.option.ConnectionUserName</name>
          <value>root</value>
          <description>username to use against metastore database</description>
        </property>

        <property>
          <name>javax.jdo.option.ConnectionPassword</name>
          <value>123456</value>
          <description>password to use against metastore database</description>
        </property>
3、导数据
    创建hive 数据库
    在C:\bigdata\apache-hive-2.3.5-src\metastore\scripts\upgrade\mysql
    将 hive-txn-schema-2.3.0.mysql.sql  在数据库工具上执行。

3、在hdfs上创建目录
    启动hadoop 启动hive。
    >hdfs dfs -mkdir /user
    >hdfs dfs -mkdir /user/hive
    >hdfs dfs -mkdir /user/hive/warehouse

    和hadoop fs -mkdir hdfs://localhost:9000/user/ Y一样。

    查看文件系统目录：
    去localhost:50070查看文件系统的目录，点击utilities -> browse the file system。

5、hive启动
6、测试
    hive>create table test_table(id INT, name string);
    hive>show tables;
==================================================================
注意：配置不成功hive。在cmd 下提示 hive 命令无法解析
根据如下应该可以配置成功，需要找个bin下面必须有cmd的文件。
https://blog.csdn.net/hawkzy/article/details/86472449


参考文章：https://blog.csdn.net/zhouzhiwengang/article/details/88191251
1、下载 选择bin文件
    http://mirror.bit.edu.cn/apache/hive/hive-2.3.5/

2、解压至C:\bigdata ，配置环境变量
3、Hive 配置文件  C:\bigdata\apache-hive-2.3.5-bin\conf
    有4个默认的配置文件模板拷贝成新的文件名

    hive-default.xml.template             ----->       hive-site.xml
    hive-env.sh.template                     ----->             hive-env.sh
    hive-exec-log4j.properties.template     ----->    hive-exec-log4j2.properties
    hive-log4j.properties.template             ----->    hive-log4j2.properties

4、新建本地目录,后面的配置文件用到
    C:\hive\apache-hive-2.1.1-bin\my_hive     

5、Hive需要调整的配置文件（hive-site.xml 和hive-env.sh）
    1、编辑 hive-site.xml
        <!--hive的临时数据目录，指定的位置在hdfs上的目录-->
        	<property>
        		<name>hive.metastore.warehouse.dir</name>
        		<value>/user/hive/warehouse</value>
        		<description>location of default database for the warehouse</description>
        	</property>

        <!--hive的临时数据目录，指定的位置在hdfs上的目录-->
        	<property>
        		<name>hive.exec.scratchdir</name>
        		<value>/tmp/hive</value>
        		<description>HDFS root scratch dir for Hive jobs which gets created with write all (733) permission. For each connecting user, an HDFS scratch dir: ${hive.exec.scratchdir}/&lt;username&gt; is created, with ${hive.scratch.dir.permission}.</description>
        	</property>

        <!-- scratchdir 本地目录 -->
        	<property>
        		<name>hive.exec.local.scratchdir</name>
        		<value>C:/hive/apache-hive-2.1.1-bin/my_hive/scratch_dir</value>
        		<description>Local scratch space for Hive jobs</description>
        	</property>

        <!-- resources_dir 本地目录 -->
        	<property>
        		<name>hive.downloaded.resources.dir</name>
        		<value>C:/hive/apache-hive-2.1.1-bin/my_hive/resources_dir/${hive.session.id}_resources</value>
        		<description>Temporary local directory for added resources in the remote file system.</description>
        	</property>

        <!-- querylog 本地目录 -->
        	<property>
        		<name>hive.querylog.location</name>
        		<value>C:/hive/apache-hive-2.1.1-bin/my_hive/querylog_dir</value>
        		<description>Location of Hive run time structured log file</description>
        	</property>

        <!-- operation_logs 本地目录 -->
        	<property>
        		<name>hive.server2.logging.operation.log.location</name>
        		<value>C:/hive/apache-hive-2.1.1-bin/my_hive/operation_logs_dir</value>
        		<description>Top level directory where operation logs are stored if logging functionality is enabled</description>
        	</property>

        <!-- 数据库连接地址配置 -->
        	<property>
        		<name>javax.jdo.option.ConnectionURL</name>
        		<value>jdbc:mysql://192.168.60.178:3306/hive?serverTimezone=UTC&amp;useSSL=false&amp;allowPublicKeyRetrieval=true</value>
        		<description>
        		JDBC connect string for a JDBC metastore.
        		</description>
        	</property>

        <!-- 数据库驱动配置 -->
        	<property>
        		<name>javax.jdo.option.ConnectionDriverName</name>
        		<value>com.mysql.cj.jdbc.Driver</value>
        		<description>Driver class name for a JDBC metastore</description>
        	</property>

        <!-- 数据库用户名 -->
        	<property>
        		<name>javax.jdo.option.ConnectionUserName</name>
        		<value>admini</value>
        		<description>Username to use against metastore database</description>
        	</property>

        <!-- 数据库访问密码 -->
        	<property>
        		<name>javax.jdo.option.ConnectionPassword</name>
        		<value>123456</value>
        		<description>password to use against metastore database</description>
        	</property>

        <!-- 解决 Caused by: MetaException(message:Version information not found in metastore. ) -->
        	<property>
        		<name>hive.metastore.schema.verification</name>
        		<value>false</value>
        		<description>
        		Enforce metastore schema version consistency.
        		True: Verify that version information stored in is compatible with one from Hive jars. Also disable automatic
        		schema migration attempt. Users are required to manually migrate schema after Hive upgrade which ensures
        		proper metastore schema migration. (Default)
        		False: Warn if the version information stored in metastore doesn't match with one from in Hive jars.
        		</description>
        	</property>

        <!-- 自动创建全部 -->
        <!-- hive Required table missing : "DBS" in Catalog""Schema" 错误 -->
        	<property>
        		<name>datanucleus.schema.autoCreateAll</name>
        		<value>true</value>
        		<description>Auto creates necessary schema on a startup if one doesn't exist. Set this to false, after creating it once.To enable auto create also set hive.metastore.schema.verification=false. Auto creation is not recommended for production use cases, run schematool command instead.</description>
        	</property>
    2、编辑 \conf\hive-env.sh 文件
        # Set HADOOP_HOME to point to a specific hadoop install directory
        export HADOOP_HOME = C:\bigdata\hadoop-2.7.7

        # Hive Configuration Directory can be controlled by:
        export HIVE_CONF_DIR=C:\bigdata\apache-hive-2.3.5-bin\conf

        # Folder containing extra libraries required for hive compilation/execution can be controlled by:
        # export HIVE_AUX_JARS_PATH=C:\bigdata\apache-hive-2.3.5-bin\lib

6、在hdfs上创建 目录

    hadoop fs  -mkdir       /tmp
    hadoop fs  -mkdir       /user/
    hadoop fs  -mkdir       /user/hive/
    hadoop fs  -mkdir       /user/hive/warehouse 
    hadoop fs  -chmod g+w   /tmp
    hadoop fs  -chmod g+w   /user/hive/warehouse

7、创建Hive 初始化依赖的数据库hive,注意编码格式：latin1
8、启动hive服务
    （1）、首先启动Hadoop，执行指令：stall-all.cmd

    （2）、Hive 初始化数据，执行指令：hive --service metastore




























