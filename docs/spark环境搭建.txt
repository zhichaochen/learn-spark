参考地址：https://note.youdao.com/ynoteshare1/index.html?id=3287f13ad5168e6d641fa260518dbeed&type=note
使用tsinghua（清华的）下载很快
scala 想要运行main 函数，需要是object
安装目录不要有空格

Windows 平台安装spark开发环境--IDEA
1、安装scala-2.11.8
    由于 scala-2.11.8 对spark 2.x 的支持最好，所以scala的版本最好选择scala-2.11.8。spark 2.x 不支持scala-2.12.x
    1、下载 scala-2.11.8 https://downloads.lightbend.com/scala/2.11.8/scala-2.11.8.zip
        解压 scala-2.11.8.zip 我这里放在E:\spark-idea
    2、配置环境变量
        SCALA_HOME = E:\spark-idea\scala-2.11.8
        Path 最后添加 E:\spark-idea\scala-2.11.8\bin
    3、验证：
        新打开一个cmd命令行，直接输入scala -version
        

2、安装java8 、安装 maven、安装 IDEA

7、配置scala 开发spark
    在main目录下手动创建scala，包名也需要手动创建

8、安装 spark 为了在本地调试spark 程序做准备，其实并不一定要安装spark
    下载 spark http://mirrors.tuna.tsinghua.edu.cn/apache/spark/spark-2.3.3/spark-2.3.3-bin-hadoop2.7.tgz 
    解压至 E:\spark-idea
    配置环境变量
    SPARK_HOME = E:\spark-idea\spark-2.3.0-bin-hadoop2.7
    在 Path 后追加%SPARK_HOME%/bin

    在任意目录下的cmd命令行中，直接执行spark-shell命令，即可开启Spark的交互式命令行模式。
======================================================================================
9、安装hadoop
　  系统变量设置后，就可以在任意当前目录下的cmd中运行spark-shell，但这个时候很有可能会碰到各种错误，
    这里主要是因为Spark是基于Hadoop的，所以这里也有必要配置一个Hadoop的运行环境。

    1、下载 
        hadoop各个版本下载： https://hadoop.apache.org/releases.html
        http://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-2.7.7/hadoop-2.7.7.tar.gz 
        解压 至 E:\spark-idea
    2、配置环境变量
        HADOOP_HOME= 
        不需要配置Path
    3、
        正常情况下是可以运行成功并进入到Spark的命令行环境下的，但是对于有些用户可能会遇到空指针的错误。
        这个时候，主要是因为Hadoop的bin目录下没有winutils.exe文件的原因造成的。这里的解决办法是： 
　　 
    - 去 https://github.com/steveloughran/winutils/blob/master/hadoop-2.7.1/bin/winutils.exe
    选择你安装的Hadoop版本号，然后进入到bin目录下，
    找到winutils.exe文件，下载方法是点击winutils.exe文件，进入之后在页面的右上方部分有一个Download按钮，点击下载即可。 

    - 下载好winutils.exe后，将这个文件放入到Hadoop的bin目录下，我这里是F:\Program Files\hadoop\bin。 
========================================================================================
参考：https://blog.csdn.net/goodmentc/article/details/80946431
10、配置hadoop文件
    1.编辑"D:\develop\hadoop-2.7.6\etc\hadoop"下的core-site.xml文件，将下列文本粘贴进去，并保存。

        <configuration>
            <property>
                <name>hadoop.tmp.dir</name>
                <value>/C:/develop/hadoop/workplace/tmp</value>
            </property>
            <property>
                <name>dfs.name.dir</name>
                <value>/C:/develop/hadoop/workplace/name</value>
            </property>
                <name>fs.default.name</name>
                <value>hdfs://localhost:9000</value>
            </property>
        </configuration>

    2、编辑“D:\develop\hadoop-2.7.6\etc\hadoop”目录下的hdfs-site.xml，粘贴以下内容并保存。
        <configuration>

            <!-- 这个参数设置为1，因为是单机版hadoop -->
            <property>
                <name>dfs.replication</name>
                <value>2</value>
            </property>
            <property>
                <name>dfs.data.dir</name>
                <value>/C:/develop/hadoop/workplace/data</value>
            </property>

        </configuration>

    3、编辑“D:\develop\hadoop-2.7.6\etc\hadoop”目录下的mapred-site.xml
    (如果不存在将mapred-site.xml.template重命名为mapred-site.xml)文件，粘贴一下内容并保存。

        <configuration>
        <property>
               <name>mapreduce.framework.name</name>
               <value>yarn</value>
            </property>
            <property>
               <name>mapred.job.tracker</name>
               <value>hdfs://localhost:9001</value>
            </property>
        </configuration>

    4.编辑“D:\develop\hadoop-2.7.6\etc\hadoop”目录下的yarn-site.xml文件，粘贴以下内容并保存。
        <configuration>
        <!-- Site specific YARN configuration properties -->

            <property>
               <name>yarn.nodemanager.aux-services</name>
               <value>mapreduce_shuffle</value>
            </property>
            <property>
               <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
               <value>org.apache.hadoop.mapred.ShuffleHandler</value>
            </property>

        </configuration>

    5、编辑“D:\dev\hadoop-2.5.2\etc\hadoop”目录下的hadoop-env.cmd文件，(在linux下是hadoop-env.sh)
        将JAVA_HOME用 @rem注释掉，编辑为JAVA_HOME的路径，然后保存；

        @rem set JAVA_HOME=%JAVA_HOME%
        set JAVA_HOME=C:\Program Files\Java\jdk1.8.0_131 --jdk安装路径


11、替换文件
    下载到的hadooponwindows-master.zip
    解压：将bin目录(包含以下.dll和.exe文件)文件替换原来hadoop目录下的bin目录；

12、运行hadoop
    1.运行cmd窗口，执行：hdfs namenode -format。
    2.运行cmd窗口，切换到hadoop的sbin目录，执行start-all.cmd，它将会启动以下4个进程窗口。
    执行2命令后，提示该命令已弃用，使用start-dfs.cmd and start-yarn.cmd

    报错1：执行的时候报错：
       系统找不到指定的路径。
       Error: JAVA_HOME is incorrectly set.
              Please update C:\bigdata\hadoop-2.7.7\conf\hadoop-env.cmd
       '-Dhadoop.security.logger' 不是内部或外部命令，也不是可运行的程序
       或批处理文件。
    原因：路径上包含了一个空格
    解决办法：
       1.用路径替代符

       C:\PROGRA~1\Java\jdk1.8.0_91
       PROGRA~1 ?=====?C:\Program Files 目录的dos文件名模式下的缩写
       长于8个字符的文件名和文件夹名，都被简化成前面6个有效字符，后面~1，有重名的就 ~2,~3,

       2.用引号括起来
        "C:\Program Files"\Java\jdk1.8.0_91 我的是：C:"\Program Files"\Java\jdk1.8.0_91

    报错2：执行时报错：
        org.xml.sax.SAXParseException; systemId: file:/C:/bigdata/hadoop-2.7.7/etc/hadoop/hdfs-site.xml;
        lineNumber: 21; columnNumber: 9; Invalid byte 2 of 2-byte UTF-8 sequence.

        原因：有一行汉字注解，去掉就可以了。
       <!-- 这个参数设置为1，因为是单机版hadoop -->
    错误3：
        yarn error：Couldn't find a package.json file
        主要原因：是本地，我使用npm安装了一个yarn，
        解决：卸载掉就可以了。

13、验证
    Resourcemanager GUI 地址 – http://localhost:8088
    Namenode GUI 地址 – http://localhost:50070
    如果这两个地址都能够打开，说明运行成功。

    第一次运行成功，第二次运行http://localhost:50070 报错：
        bindexception address already in use:0.0.0.0:50070

        查看端口50070： netstat -ano|findstr 50070
        发现端口并没有被占用。
        按照网上的方法：

        暂停所有Hadoop进程；
        清除core-site.xml、hdfs-site.xml文件中配置的所有目录文件
        重新hdfs namenode -format
        重新运行D:\develop\hadoop-2.7.6\sbin/start-all.cmd，结果还是报错。
        怀疑是其他软件影响导致，于是，关闭了360卫士和360杀毒等软件，保留了谷歌浏览器。 重复上述方法，OK啦！

14、上传文件测试
    1、创建输入目录
        hadoop fs -mkdir hdfs://localhost:9000/user/

    2、上传数据到目录
        hadoop fs -put C:\bigdata\student.json hdfs://localhost:9000/user/wcinput

    3、查看文件
        终端执行命令：hadoop fs -ls hdfs://localhost:9000/user/wcinput
        Found 1 items：如下内容
        -rw-r--r--   1 admin supergroup        103 2019-06-29 15:45 hdfs://localhost:9000/user/wcinput

    说明文件已经存在：










